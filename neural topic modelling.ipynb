{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Word2Vec model (trained on an enormous Google corpus)\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path,documents,reviewColumn):\n",
    "    # read review texts\n",
    "    reviews = []\n",
    "    for name in documents:\n",
    "        raw_data = pd.read_csv(path+name+\".csv\")\n",
    "        raw_data = raw_data.dropna(subset=[reviewColumn])\n",
    "        raw_data = raw_data.reset_index(drop=True)\n",
    "        reviews += list(raw_data[reviewColumn])\n",
    "    sentences = []\n",
    "    for review in reviews:\n",
    "        #split text into sentences (separated by .,?,!, or a newline)\n",
    "        sentences += re.split(r\"[\\.\\?!]+[ \\n]*\",review)\n",
    "    #split sentences into words\n",
    "    tokenized = [re.split(r\"[,]*[ \\n]+[,]*\",sentence) for sentence in sentences]\n",
    "    #remove empty sentences\n",
    "    tokenized = [element for element in tokenized if element!=['']]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(word,vec_model):\n",
    "    try:\n",
    "        return vec_model[word]\n",
    "    except KeyError:\n",
    "        return np.zeros((300,1))\n",
    "\n",
    "def vectorize(sentence_text,vec_model):\n",
    "    return [tf.reshape(tf.convert_to_tensor(word2vec(word,vec_model),dtype=tf.float32),[300,1]) for word in sentence_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence,M):\n",
    "    #calculate how much the model should focus on each word\n",
    "    a = embedding_focus(sentence,M)\n",
    "    #sentence embedding is a weighted average of the vectors in the sentence\n",
    "    z_s=tf.reduce_sum([a[i]*sentence[i] for i in range(len(sentence))],axis=0)\n",
    "    return z_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_focus(sentence,M):\n",
    "    y_s = tf.reshape(tf.reduce_mean(sentence,axis=0),[300,1])\n",
    "    a = tf.convert_to_tensor([tf.matmul(tf.matmul(tf.transpose(word),M),y_s) for word in sentence])\n",
    "    \n",
    "    #reduce values to prevent overflow\n",
    "    a -= tf.reduce_max(a)-70\n",
    "    #apply softmax\n",
    "    a = tf.exp(a)\n",
    "    a /= tf.reduce_sum(a)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_mean(sentence):\n",
    "    y_s = tf.reduce_mean(sentence,axis=0)\n",
    "    return y_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction(embedding,W,b,T):\n",
    "    #calculate the topic proportion of the sentence based on the embedding and learned parameters\n",
    "    p_t = topic_proportion(embedding,W,b,T)\n",
    "    #get a weighted average of the topic vectors\n",
    "    r_s = tf.matmul(tf.transpose(T),p_t)\n",
    "    return r_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_proportion(embedding,W,b,T):\n",
    "    #multiply by M and add bias b (both learned) to get prominance scores\n",
    "    p_t = tf.matmul(W,embedding)+b\n",
    "    #reduce values to prevent overflow\n",
    "    p_t -= tf.reduce_max(p_t)-70\n",
    "    #apply softmax\n",
    "    p_t = tf.exp(p_t)\n",
    "    p_t /= tf.reduce_sum(p_t)\n",
    "    return p_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_s(embedding,re_embedding,negatives):\n",
    "    #calculate cosine similarity of reconstruction with embedding and a random sample of other embeddings\n",
    "    cos_sim = tf.reshape(tf.matmul(tf.transpose(embedding),re_embedding),[1])\n",
    "    neg_sim = tf.convert_to_tensor([tf.reshape(tf.matmul(tf.transpose(re_embedding),neg),[1]) for neg in negatives])\n",
    "    #loss tries to maximize cosine similarity of reconstruction with embedding \n",
    "    #while minimizing similarity with other (generally unrelated) sentence embeddings\n",
    "    return tf.reduce_sum(tf.maximum(0,1-cos_sim+neg_sim))\n",
    "\n",
    "def J(embeddings,reconstructions,negs):\n",
    "    total_loss = 0\n",
    "    dJ_dr_list = []\n",
    "    #get loss for each sentence and sum them\n",
    "    for i in range(len(embeddings)):\n",
    "        temp = J_s(embeddings[i],reconstructions[i],negs)\n",
    "        total_loss += temp\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_topics(seed_lists,seed_weights,unseeded):\n",
    "    \n",
    "    #perform sparsemax of raw seed weights\n",
    "    weights_sorted = [tf.sort(element,direction='DESCENDING') for element in seed_weights]\n",
    "    k = [tf.range(element.shape[0],dtype='float32')+1 for element in seed_weights]\n",
    "    k_array = [1 + tf.math.multiply(k[i],weights_sorted[i]) for i in range(len(k))]\n",
    "    weights_cumsum = [tf.cumsum(element) for element in weights_sorted]\n",
    "    k_selected = [k_array[i] > weights_cumsum[i] for i in range(len(k_array))]\n",
    "    k_max = [tf.reduce_max(tf.where(element)).numpy()+1 for element in k_selected]\n",
    "    threshold = [(weights_cumsum[i][k_max[i]-1] - 1) / k_max[i] for i in range(len(weights_cumsum))]\n",
    "    seed_weights = [tf.maximum(seed_weights[i]-threshold[i],0) for i in range(len(seed_weights))]\n",
    "    \n",
    "    #seeded topoics are just weights averages of seed word vectors\n",
    "    seed_topics = tf.concat([tf.reshape(tf.matmul(tf.transpose(seed_lists[i]),seed_weights[i]),[1,300]) for i in range(len(seed_lists))],0)\n",
    "    all_topics = tf.concat([seed_topics,unseeded],0)\n",
    "    return all_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(vectorized_reviews,M,W,b,seed_lists,z,uT,m,negative_pool):\n",
    "    with tf.GradientTape(persistent=True) as g:\n",
    "        #track gradients\n",
    "        g.watch(M)\n",
    "        g.watch(W)\n",
    "        g.watch(b)\n",
    "        g.watch(z)\n",
    "        g.watch(uT)\n",
    "        T = build_topics(seed_lists,z,uT)\n",
    "        #get sentence embeddings\n",
    "        sentence_embeddings = [sentence_embedding(sentence,M) for sentence in vectorized_reviews]\n",
    "        #get sentence reconstructions\n",
    "        sentence_reconstructions = [reconstruction(embed,W,b,T) for embed in sentence_embeddings]\n",
    "        #choose random negative sentences\n",
    "        negs = random.sample(negative_pool,m)\n",
    "        #calculate loss over minibatch\n",
    "        total_loss = J(sentence_embeddings,sentence_reconstructions,negs)\n",
    "    grads = g.gradient(total_loss,{'M':M,'W':W,'b':b,'z':z,'uT':uT})\n",
    "    #return loss and gradients\n",
    "    return total_loss,grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_epoch(vectorized_reviews,M,W,b,seed_lists,z,uT,batch_size=50,learning_rate=0.001,m=20,beta_1=0.9,beta_2=0.999,epsilon=1e-8,num_epochs=1):\n",
    "    t=0\n",
    "    #initialize adam optimizer values to 0\n",
    "    m_M = tf.zeros(M.shape)\n",
    "    m_W = tf.zeros(W.shape)\n",
    "    m_b = tf.zeros(b.shape)\n",
    "    m_z = [tf.zeros(z[i].shape) for i in range(len(z))]\n",
    "    m_uT = tf.zeros(uT.shape)\n",
    "    v_M = tf.zeros(M.shape)\n",
    "    v_W = tf.zeros(W.shape)\n",
    "    v_b = tf.zeros(b.shape)\n",
    "    v_z = [tf.zeros(z[i].shape) for i in range(len(z))]\n",
    "    v_uT = tf.zeros(uT.shape)\n",
    "    #calculate the pool of negative sentences\n",
    "    #we use a straight mean rather than a full embedding\n",
    "    negative_pool = [sentence_mean(sentence) for sentence in vectorized_reviews]\n",
    "    for j in range(num_epochs):\n",
    "        #randomize order to avoid overfitting\n",
    "        random.shuffle(vectorized_reviews)\n",
    "        for i in range(0,len(vectorized_reviews),batch_size):\n",
    "            t+=1\n",
    "            \n",
    "            #get gradients through a forward pass with a minibatch\n",
    "            loss,grads=forward_pass(vectorized_reviews[i:min(len(vectorized_reviews),i+batch_size)],M,W,b,seed_lists,z,uT,m,negative_pool)\n",
    "            #update adam optimizer values\n",
    "            m_M = beta_1*m_M+(1-beta_1)*grads['M']\n",
    "            m_W = beta_1*m_W+(1-beta_1)*grads['W']\n",
    "            m_b = beta_1*m_b+(1-beta_1)*grads['b']\n",
    "            m_z = [beta_1*m_z[i]+(1-beta_1)*grads['z'][i] for i in range(len(z))]\n",
    "            m_uT = beta_1*m_uT+(1-beta_1)*grads['uT']\n",
    "\n",
    "            v_M = beta_2*v_M+(1-beta_2)*tf.square(grads['M'])\n",
    "            v_W = beta_2*v_W+(1-beta_2)*tf.square(grads['W'])\n",
    "            v_b = beta_2*v_b+(1-beta_2)*tf.square(grads['b'])\n",
    "            v_z = [beta_2*v_z[i]+(1-beta_2)*tf.square(grads['z'][i]) for i in range(len(z))]\n",
    "            v_uT = beta_2*v_uT+(1-beta_2)*tf.square(grads['uT'])\n",
    "            \n",
    "            #update parameters\n",
    "            M-=(m_M/(1-beta_1**t))/(tf.sqrt(v_M/(1-beta_2**t))+epsilon)*learning_rate\n",
    "            W-=(m_W/(1-beta_1**t))/(tf.sqrt(v_W/(1-beta_2**t))+epsilon)*learning_rate\n",
    "            b-=(m_b/(1-beta_1**t))/(tf.sqrt(v_b/(1-beta_2**t))+epsilon)*learning_rate\n",
    "            z=[z[i]-(m_z[i]/(1-beta_1**t))/(tf.sqrt(v_z[i]/(1-beta_2**t))+epsilon)*learning_rate for i in range(len(z))]\n",
    "            uT-=(m_uT/(1-beta_1**t))/(tf.sqrt(v_uT/(1-beta_2**t))+epsilon)*learning_rate\n",
    "    #return learned parameters\n",
    "    return M,W,b,z,uT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = preprocess(\"input/\",'reviews','companyReviewText')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_reviews = [vectorize(review,model) for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMUNSEEDED = 1\n",
    "NUMSEEDED = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize parameters with small random values\n",
    "M = tf.random.uniform([300,300],minval=-1)\n",
    "b = tf.random.uniform([NUMSEEDED+NUMUNSEEDED,1],minval=-1)\n",
    "W = tf.random.uniform([NUMSEEDED+NUMUNSEEDED,300],minval=-1)\n",
    "#seed words\n",
    "seed_words = [[\"customs\",\"international\",\"country\"],\n",
    "             [\"service\",\"driver\"],\n",
    "             [\"paid\",\"worth\",\"fee\"],\n",
    "             [\"delivery\",\"package\",\"shipment\"],\n",
    "             [\"speed\",\"quick\",\"late\"],\n",
    "             [\"tracking\",\"email\",\"website\"]]\n",
    "seed_lists = [tf.concat([tf.reshape(model[word],[1,300]) for word in seed_words[i]],0) for i in range(len(seed_words))]\n",
    "seed_weights = [tf.random.uniform([len(seed_lists[i]),1]) for i in range(len(seed_lists))]\n",
    "seed_topics = tf.concat([tf.reshape(tf.matmul(tf.transpose(seed_lists[i]),seed_weights[i]),[1,300]) for i in range(len(seed_lists))],0)\n",
    "unseeded = tf.random.uniform([NUMUNSEEDED,300],minval=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "M,W,b,seed_weights,unseeded = training_epoch(vectorized_reviews,M,W,b,seed_lists,seed_weights,unseeded,num_epochs=2)\n",
    "T = build_topics(seed_lists,seed_weights,unseeded)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
