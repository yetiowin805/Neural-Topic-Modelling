{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22d45701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_GPU_ALLOCATOR=cuda_malloc_async\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-19 16:00:25.754271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 16:00:25.755072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 16:00:25.764539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 16:00:25.765259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 16:00:25.765973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 16:00:25.766641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import codecs\n",
    "\n",
    "from importlib import reload\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')\n",
    "\n",
    "%env TF_GPU_ALLOCATOR=cuda_malloc_async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3c5d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Word2Vec model (trained on an enormous Google corpus)\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d3cfac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path,documents,reviewColumn):\n",
    "    # read review texts\n",
    "    reviews = []\n",
    "    for name in documents:\n",
    "        raw_data = pd.read_csv(path+name+\".csv\")\n",
    "        raw_data = raw_data.dropna(subset=[reviewColumn])\n",
    "        raw_data = raw_data.reset_index(drop=True)\n",
    "        reviews += list(raw_data[reviewColumn])\n",
    "    sentences = []\n",
    "    for review in reviews:\n",
    "        #split text into sentences (separated by .,?,!, or a newline)\n",
    "        sentences += re.split(r\"[\\.\\?!]+[ \\n]*\",review)\n",
    "    #split sentences into words\n",
    "    tokenized = [re.split(r\"[,]*[ \\n]+[,]*\",sentence) for sentence in sentences]\n",
    "    #remove empty sentences\n",
    "    tokenized = [element for element in tokenized if element!=['']]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1208a4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(word,vec_model):\n",
    "    try:\n",
    "        return vec_model[word]\n",
    "    except KeyError:\n",
    "        return np.zeros((300,1))\n",
    "\n",
    "def vectorize(sentence_text,vec_model):\n",
    "    return [tf.reshape(tf.convert_to_tensor(word2vec(word,vec_model),dtype=tf.float32),[300,1]) for word in sentence_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "811dd0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence,M):\n",
    "    #calculate how much the model should focus on each word\n",
    "    a = embedding_focus(sentence,M)\n",
    "    #sentence embedding is a weighted average of the vectors in the sentence\n",
    "    z_s=tf.reduce_sum([a[i]*sentence[i] for i in range(len(sentence))],axis=0)\n",
    "    return z_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8ff52ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_focus(sentence,M):\n",
    "    y_s = tf.reshape(tf.reduce_mean(sentence,axis=0),[300,1])\n",
    "    a = tf.convert_to_tensor([tf.matmul(tf.matmul(tf.transpose(word),M),y_s) for word in sentence])\n",
    "    \n",
    "    #reduce values to prevent overflow\n",
    "    a -= tf.reduce_max(a)-70\n",
    "    #apply softmax\n",
    "    a = tf.exp(a)\n",
    "    a /= tf.reduce_sum(a)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b12fbdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_mean(sentence):\n",
    "    y_s = tf.reduce_mean(sentence,axis=0)\n",
    "    return y_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80facfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction(embedding,W,b,T):\n",
    "    #calculate the topic proportion of the sentence based on the embedding and learned parameters\n",
    "    p_t = topic_proportion(embedding,W,b,T)\n",
    "    #get a weighted average of the topic vectors\n",
    "    r_s = tf.matmul(tf.transpose(T),p_t)\n",
    "    return r_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdb160b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_proportion(embedding,W,b,T):\n",
    "    #multiply by M and add bias b (both learned) to get prominance scores\n",
    "    p_t = tf.matmul(W,embedding)+b\n",
    "    #reduce values to prevent overflow\n",
    "    p_t -= tf.reduce_max(p_t)-70\n",
    "    #apply softmax\n",
    "    p_t = tf.exp(p_t)\n",
    "    p_t /= tf.reduce_sum(p_t)\n",
    "    return p_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b224c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_s(embedding,re_embedding,negatives):\n",
    "    #calculate cosine similarity of reconstruction with embedding and a random sample of other embeddings\n",
    "    cos_sim = tf.reshape(tf.matmul(tf.transpose(embedding),re_embedding),[1])\n",
    "    neg_sim = tf.convert_to_tensor([tf.reshape(tf.matmul(tf.transpose(re_embedding),neg),[1]) for neg in negatives])\n",
    "    #loss tries to maximize cosine similarity of reconstruction with embedding \n",
    "    #while minimizing similarity with other (generally unrelated) sentence embeddings\n",
    "    return tf.reduce_sum(tf.maximum(0,1-cos_sim+neg_sim))\n",
    "\n",
    "def J(embeddings,reconstructions,negs):\n",
    "    total_loss = 0\n",
    "    dJ_dr_list = []\n",
    "    #get loss for each sentence and sum them\n",
    "    for i in range(len(embeddings)):\n",
    "        temp = J_s(embeddings[i],reconstructions[i],negs)\n",
    "        total_loss += temp\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bda67747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no longer used\n",
    "def regularization(T):\n",
    "    rows = []\n",
    "    #normalize topic vectors to length 1\n",
    "    for i in range(T.shape[0]):\n",
    "        rows.append([T[i]/tf.reduce_sum(tf.square(T[i]))])\n",
    "    normed = tf.concat(axis=0, values=rows)\n",
    "    #get dot product (=cosine similarity) of all pairs of topics\n",
    "    dots = tf.matmul(normed,tf.transpose(normed))\n",
    "    #subtract identity matrix (since the cosine similarity of all topics with itself is 1)\n",
    "    U = dots-tf.eye(T.shape[0])\n",
    "    #get norm of entire matrix\n",
    "    return tf.reduce_sum(tf.square(U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49584a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_topics(seed_lists,seed_weights,unseeded):\n",
    "    \n",
    "    #perform sparsemax of raw seed weights\n",
    "    weights_sorted = [tf.sort(element,direction='DESCENDING') for element in seed_weights]\n",
    "    k = [tf.range(element.shape[0],dtype='float32')+1 for element in seed_weights]\n",
    "    k_array = [1 + tf.math.multiply(k[i],weights_sorted[i]) for i in range(len(k))]\n",
    "    weights_cumsum = [tf.cumsum(element) for element in weights_sorted]\n",
    "    k_selected = [k_array[i] > weights_cumsum[i] for i in range(len(k_array))]\n",
    "    k_max = [tf.reduce_max(tf.where(element)).numpy()+1 for element in k_selected]\n",
    "    threshold = [(weights_cumsum[i][k_max[i]-1] - 1) / k_max[i] for i in range(len(weights_cumsum))]\n",
    "    seed_weights = [tf.maximum(seed_weights[i]-threshold[i],0) for i in range(len(seed_weights))]\n",
    "    \n",
    "    #seeded topoics are just weights averages of seed word vectors\n",
    "    seed_topics = tf.concat([tf.reshape(tf.matmul(tf.transpose(seed_lists[i]),seed_weights[i]),[1,300]) for i in range(len(seed_lists))],0)\n",
    "    all_topics = tf.concat([seed_topics,unseeded],0)\n",
    "    return all_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84541ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"0\",\"dhl\",\"fedex\",\"ups\",\"usps\"]\n",
    "files += [\"sj_\"+str(i) for i in range(1,7)]\n",
    "files += [\"tp_\"+str(i) for i in range(1,6)]\n",
    "reviews = preprocess(\"input/shipping/\",files,'reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0042f05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reviews_google = []\\nfor i in range(2,6):\\n    f = open(\"input/shipping_google_\"+str(i)+\".json\", encoding=\\'utf-8\\')\\n    raw = json.load(f)\\n    for macro in raw:\\n        for element in macro[\"reviews\"]:\\n            if element[\"text\"]!=None:\\n                reviews_google.append(element[\"text\"])\\nsentences=[]\\nfor review in reviews_google:\\n    sentences += re.split(r\"[\\\\.\\\\?!]+[ \\n]*\",review)\\ntokenized = [re.split(r\"[,]*[ \\n]+[,]*\",sentence) for sentence in sentences]\\ntokenized = [element for element in tokenized if element!=[\\'\\']]\\nreviews=tokenized'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"reviews_google = []\n",
    "for i in range(2,6):\n",
    "    f = open(\"input/shipping_google_\"+str(i)+\".json\", encoding='utf-8')\n",
    "    raw = json.load(f)\n",
    "    for macro in raw:\n",
    "        for element in macro[\"reviews\"]:\n",
    "            if element[\"text\"]!=None:\n",
    "                reviews_google.append(element[\"text\"])\n",
    "sentences=[]\n",
    "for review in reviews_google:\n",
    "    sentences += re.split(r\"[\\.\\?!]+[ \\n]*\",review)\n",
    "tokenized = [re.split(r\"[,]*[ \\n]+[,]*\",sentence) for sentence in sentences]\n",
    "tokenized = [element for element in tokenized if element!=['']]\n",
    "reviews=tokenized\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b324af",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_reviews = [vectorize(review,model) for review in tqdm(reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e0f7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMUNSEEDED = 1\n",
    "NUMSEEDED = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43ed22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize parameters with small random values\n",
    "M = tf.random.uniform([300,300],minval=-1)\n",
    "b = tf.random.uniform([NUMSEEDED+NUMUNSEEDED,1],minval=-1)\n",
    "W = tf.random.uniform([NUMSEEDED+NUMUNSEEDED,300],minval=-1)\n",
    "#seed words\n",
    "seed_words = [[\"customs\",\"international\",\"country\"],\n",
    "             [\"service\",\"driver\"],\n",
    "             [\"paid\",\"worth\",\"fee\"],\n",
    "             [\"delivery\",\"package\",\"shipment\"],\n",
    "             [\"speed\",\"quick\",\"late\"],\n",
    "             [\"tracking\",\"email\",\"website\"]]\n",
    "seed_lists = [tf.concat([tf.reshape(model[word],[1,300]) for word in seed_words[i]],0) for i in range(len(seed_words))]\n",
    "seed_weights = [tf.random.uniform([len(seed_lists[i]),1]) for i in range(len(seed_lists))]\n",
    "seed_topics = tf.concat([tf.reshape(tf.matmul(tf.transpose(seed_lists[i]),seed_weights[i]),[1,300]) for i in range(len(seed_lists))],0)\n",
    "unseeded = tf.random.uniform([NUMUNSEEDED,300],minval=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306deab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(vectorized_reviews,M,W,b,seed_lists,z,uT,m,negative_pool):\n",
    "    with tf.GradientTape(persistent=True) as g:\n",
    "        #track gradients\n",
    "        g.watch(M)\n",
    "        g.watch(W)\n",
    "        g.watch(b)\n",
    "        g.watch(z)\n",
    "        g.watch(uT)\n",
    "        T = build_topics(seed_lists,z,uT)\n",
    "        #get sentence embeddings\n",
    "        sentence_embeddings = [sentence_embedding(sentence,M) for sentence in vectorized_reviews]\n",
    "        #get sentence reconstructions\n",
    "        sentence_reconstructions = [reconstruction(embed,W,b,T) for embed in sentence_embeddings]\n",
    "        #choose random negative sentences\n",
    "        negs = random.sample(negative_pool,m)\n",
    "        #calculate loss over minibatch\n",
    "        total_loss = J(sentence_embeddings,sentence_reconstructions,negs)\n",
    "    grads = g.gradient(total_loss,{'M':M,'W':W,'b':b,'z':z,'uT':uT})\n",
    "    #return loss and gradients\n",
    "    return total_loss,grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f14388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_epoch(vectorized_reviews,M,W,b,seed_lists,z,uT,batch_size=50,learning_rate=0.001,m=20,beta_1=0.9,beta_2=0.999,epsilon=1e-8,num_epochs=1):\n",
    "    t=0\n",
    "    #initialize adam optimizer values to 0\n",
    "    m_M = tf.zeros(M.shape)\n",
    "    m_W = tf.zeros(W.shape)\n",
    "    m_b = tf.zeros(b.shape)\n",
    "    m_z = [tf.zeros(z[i].shape) for i in range(len(z))]\n",
    "    m_uT = tf.zeros(uT.shape)\n",
    "    v_M = tf.zeros(M.shape)\n",
    "    v_W = tf.zeros(W.shape)\n",
    "    v_b = tf.zeros(b.shape)\n",
    "    v_z = [tf.zeros(z[i].shape) for i in range(len(z))]\n",
    "    v_uT = tf.zeros(uT.shape)\n",
    "    #calculate the pool of negative sentences\n",
    "    #we use a straight mean rather than a full embedding\n",
    "    negative_pool = [sentence_mean(sentence) for sentence in tqdm(vectorized_reviews)]\n",
    "    for j in range(num_epochs):\n",
    "        #randomize order to avoid overfitting\n",
    "        random.shuffle(vectorized_reviews)\n",
    "        for i in tqdm(range(0,len(vectorized_reviews),batch_size)):\n",
    "            t+=1\n",
    "            \n",
    "            #get gradients through a forward pass with a minibatch\n",
    "            loss,grads=forward_pass(vectorized_reviews[i:min(len(vectorized_reviews),i+batch_size)],M,W,b,seed_lists,z,uT,m,negative_pool)\n",
    "            #update adam optimizer values\n",
    "            m_M = beta_1*m_M+(1-beta_1)*grads['M']\n",
    "            m_W = beta_1*m_W+(1-beta_1)*grads['W']\n",
    "            m_b = beta_1*m_b+(1-beta_1)*grads['b']\n",
    "            m_z = [beta_1*m_z[i]+(1-beta_1)*grads['z'][i] for i in range(len(z))]\n",
    "            m_uT = beta_1*m_uT+(1-beta_1)*grads['uT']\n",
    "\n",
    "            v_M = beta_2*v_M+(1-beta_2)*tf.square(grads['M'])\n",
    "            v_W = beta_2*v_W+(1-beta_2)*tf.square(grads['W'])\n",
    "            v_b = beta_2*v_b+(1-beta_2)*tf.square(grads['b'])\n",
    "            v_z = [beta_2*v_z[i]+(1-beta_2)*tf.square(grads['z'][i]) for i in range(len(z))]\n",
    "            v_uT = beta_2*v_uT+(1-beta_2)*tf.square(grads['uT'])\n",
    "            \n",
    "            #update parameters\n",
    "            M-=(m_M/(1-beta_1**t))/(tf.sqrt(v_M/(1-beta_2**t))+epsilon)*learning_rate\n",
    "            W-=(m_W/(1-beta_1**t))/(tf.sqrt(v_W/(1-beta_2**t))+epsilon)*learning_rate\n",
    "            b-=(m_b/(1-beta_1**t))/(tf.sqrt(v_b/(1-beta_2**t))+epsilon)*learning_rate\n",
    "            z=[z[i]-(m_z[i]/(1-beta_1**t))/(tf.sqrt(v_z[i]/(1-beta_2**t))+epsilon)*learning_rate for i in range(len(z))]\n",
    "            uT-=(m_uT/(1-beta_1**t))/(tf.sqrt(v_uT/(1-beta_2**t))+epsilon)*learning_rate\n",
    "    #return learned parameters\n",
    "    return M,W,b,z,uT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4b4bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "M,W,b,seed_weights,unseeded = training_epoch(vectorized_reviews,M,W,b,seed_lists,seed_weights,unseeded,num_epochs=2)\n",
    "T = build_topics(seed_lists,seed_weights,unseeded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b32a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[7666]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d445a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_sentence = vectorized_reviews[7666]\n",
    "temp=sentence_embedding(temp_sentence,M)\n",
    "#temp=reconstruction(temp,W,b,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b3340",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_focus(temp_sentence,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce9a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_proportion(temp,W,b,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a5f82ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20903/1458684081.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvectorized_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msentence_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_reviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msentence_reconstructions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtopic_proportion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0membed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "#output\n",
    "vectorized_reviews = [vectorize(review,model) for review in tqdm(reviews)]\n",
    "sentence_embeddings = [sentence_embedding(sentence,M) for sentence in tqdm(vectorized_reviews)]\n",
    "sentence_reconstructions = [topic_proportion(embed,W,b,T) for embed in tqdm(sentence_embeddings)]\n",
    "\n",
    "for i in range(NUMSEEDED+NUMUNSEEDED):\n",
    "    file = codecs.open('output/shipping_topic_'+str(i)+\".txt\",'w','utf-8')\n",
    "    for j in range(len(reviews)):\n",
    "        if sentence_reconstructions[j][i]>=0.4:\n",
    "            file.write(str(j)+' '+ ' '.join(reviews[j]) +' '+str(sentence_reconstructions[j][i].numpy())+'\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d45bcef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cause', 0.26846811175346375), ('cer_tainly', 0.26693272590637207), ('anythng', 0.2646339535713196), ('tings', 0.2591475248336792), ('Adam_Giambrone', 0.2571435272693634)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=[np.array(T[6])],topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3648fd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       " array([[0.36382526],\n",
       "        [0.6604932 ],\n",
       "        [0.01158832]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       " array([[1.2064757 ],\n",
       "        [0.21059214]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       " array([[0.52564836],\n",
       "        [0.00380291],\n",
       "        [0.3745941 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       " array([[ 0.70229983],\n",
       "        [-0.3899485 ],\n",
       "        [ 0.40088555]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       " array([[0.845086  ],\n",
       "        [0.18793581],\n",
       "        [0.5290985 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       " array([[0.25600356],\n",
       "        [0.9263149 ],\n",
       "        [0.53256124]], dtype=float32)>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,grads=forward_pass(vectorized_reviews,M,W,b,seed_lists,seed_weights,unseeded,20,[sentence_mean(sentence) for sentence in vectorized_reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beea077",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21885eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c72c442",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
